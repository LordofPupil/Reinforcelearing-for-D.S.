{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model free prediction Methos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d5cc259e26d659"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def join(str1, str2): # 这是一个简单的字符串拼接算法\n",
    "    return str1 + '-' + str2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T08:12:14.220442Z",
     "start_time": "2025-01-04T08:12:14.217875Z"
    }
   },
   "id": "6563b04116109170",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](Images/image1.png)\n",
    "![image](Images/image2.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da31ec6b735f6f07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面是对一个简单的MDP过程的基本信息做一些定义"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba6fe23ea4aa2809"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# 定义状态转移概率矩阵P\n",
    "P = [\n",
    "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
    "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
    "]\n",
    "P = np.array(P)\n",
    "\n",
    "rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数\n",
    "gamma = 0.5  # 定义折扣因子\n",
    "\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
    "# 状态转移函数\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "# 奖励函数\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "gamma = 0.5  # 折扣因子\n",
    "MDP = (S, A, P, R, gamma)\n",
    "\n",
    "# 策略1,随机策略\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "# 策略2\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-04T08:12:17.568934Z",
     "start_time": "2025-01-04T08:12:17.552892Z"
    }
   },
   "id": "644b0426503ac0e3",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 蒙特卡洛方法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30a91e175c7471f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 蒙特卡洛方法的采样的实现\n",
    "\n",
    "首先进行状态的采样"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fd998ff3f06124b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sample(MDP, Pi, timestep_max, number):\n",
    "    \"\"\"\n",
    "    这个函数是根据一个已知的MDP环境进行采样的算法\n",
    "    每个episode的组织方法是每一步都是一个元组（s,a,r,s_next）即当前状态,执行的动作,得到的奖励,以及下一步的状态\n",
    "    :param MDP: MDP的基本参数, 这里是模拟环境, 在真实环境下没有状态转移矩阵等\n",
    "    :param Pi: 具体的策略\n",
    "    :param timestep_max: 限制最大的时间步\n",
    "    :param number: 总共需要采样的序列数\n",
    "    :return: 返回值为采样好的episode\n",
    "    \"\"\"\n",
    "    S, A, P, R, gamma = MDP\n",
    "    episodes = []\n",
    "    for _ in range(number):\n",
    "        episode = []\n",
    "        timestep = 0\n",
    "        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n",
    "        # 当前状态为终止状态或者时间步太长时,一次采样结束\n",
    "        while s != \"s5\" and timestep <= timestep_max:\n",
    "            timestep += 1\n",
    "            # 在状态s下根据策略选择动作, 因为是策略是一个随机分布下面的代码是根据随机成成一个动作\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            for a_opt in A:\n",
    "                temp += Pi.get(join(s, a_opt), 0) # 从策略字典中查询在某一状态下执行某一动作的概率, 如果没有这个则返回0\n",
    "                if temp > rand:\n",
    "                    a = a_opt\n",
    "                    r = R.get(join(s, a), 0)\n",
    "                    break\n",
    "            # 根据状态转移概率得到下一个状态s_next\n",
    "            rand, temp = np.random.rand(), 0\n",
    "            for s_opt in S:\n",
    "                temp += P.get(join(join(s, a), s_opt), 0)# 在s状态下执行a动作到达s_opt的概率\n",
    "                if temp > rand:\n",
    "                    s_next = s_opt\n",
    "                    break\n",
    "            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n",
    "            s = s_next  # s_next变成当前状态,开始接下来的循环\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:05:57.784647Z",
     "start_time": "2025-01-05T02:05:57.754010Z"
    }
   },
   "id": "6032449705495dea",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一条序列\n",
      " [('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n",
      "第二条序列\n",
      " [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '保持s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n",
      "第五条序列\n",
      " [('s4', '概率前往', 1, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n"
     ]
    }
   ],
   "source": [
    "episodes = sample(MDP, Pi_1, 20, 5)\n",
    "print('第一条序列\\n', episodes[0])\n",
    "print('第二条序列\\n', episodes[1])\n",
    "print('第五条序列\\n', episodes[4])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:05:58.218180Z",
     "start_time": "2025-01-05T02:05:58.186250Z"
    }
   },
   "id": "156dc71d2cb8f6ab",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 蒙特卡洛方法主体的实现\n",
    "\n",
    "我们采用累进更新形式的蒙特卡洛算法其公式为:\n",
    "$$V(S_t)\\leftarrow V(S_t)+\\frac{1}{N(S_t)}\\left(G_t-V(S_t)\\right)$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8bc138fa257c836"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 对所有采样序列计算所有状态的价值\n",
    "def MC(episodes, V, N, gamma):\n",
    "    \"\"\"\n",
    "    :param episodes: 采样好的episode\n",
    "    :param V: 状态的价值函数应该是一个一维的向量\n",
    "    :param N: 状态的计数器应该是一个矩阵\n",
    "    :param gamma: 衰减因子\n",
    "    :return: null\n",
    "    \"\"\"\n",
    "    for episode in episodes:\n",
    "        G = 0\n",
    "        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            G = r + gamma * G\n",
    "            N[s] = N[s] + 1\n",
    "            V[s] = V[s] + (G - V[s]) / N[s]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:07:17.124759Z",
     "start_time": "2025-01-05T02:07:17.115270Z"
    }
   },
   "id": "2407ef3592de10be",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用蒙特卡洛方法计算MDP的状态价值为\n",
      " {'s1': -1.1921303313152458, 's2': -1.660432153769328, 's3': 0.5369444492885036, 's4': 6.081644157323337, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "MC(episodes, V, N, gamma)\n",
    "print(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:07:17.810441Z",
     "start_time": "2025-01-05T02:07:17.801850Z"
    }
   },
   "id": "20afcf639f1d38f2",
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 时序差分算法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4b8f63d599f777f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TD(0)时序差分算法\n",
    "\n",
    "TD(O)时序差分算法的迭代公式为:\n",
    "$$V(S_t)\\leftarrow V(S_t)+\\alpha\\left({R_{t+1}+\\gamma V(S_{t+1})}-V(S_t)\\right)$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9a536b758259e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def TDZero(episodes, V, N, gamma, alpha=1.0):\n",
    "    \"\"\"\n",
    "    :param episodes: 采样好的episode\n",
    "    :param V: 状态的价值函数应该是一个一维的向量\n",
    "    :param N: 状态的计数器应该是一个矩阵\n",
    "    :param gamma: 衰减因子\n",
    "    :return: null\n",
    "    \"\"\"\n",
    "    for episode in episodes:\n",
    "        for i in range(len(episode)):  #一个序列从后往前计算\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            V[s] = V[s] + alpha*(r + gamma*V[s_next] - V[s]) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:08:42.884456Z",
     "start_time": "2025-01-05T02:08:42.858332Z"
    }
   },
   "id": "ea223e6ddc4ea241",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用TD(0)方法计算MDP的状态价值为\n",
      " {'s1': -1.2284732210578542, 's2': -1.8065500547063014, 's3': -0.019549975641035618, 's4': 6.773078763112501, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 1000)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "TDZero(episodes, V, N, gamma, alpha=0.5)\n",
    "print(\"使用TD(0)方法计算MDP的状态价值为\\n\", V)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:08:45.338189Z",
     "start_time": "2025-01-05T02:08:45.315697Z"
    }
   },
   "id": "1e29b64bf8e2e6b5",
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $TD(\\lambda)$算法\n",
    "\n",
    "$TD(\\lambda)$需要对每一个状态维护一个资格迹:\n",
    "$$E_t(s)=\\gamma \\lambda E_{t-1}(s)+\\mathbf{1}(S_t=s)$$\n",
    "其迭代公式为:\n",
    "$$V(s) \\leftarrow V(s)+\\alpha\\delta_tE_t(s)$$\n",
    "其中$\\delta_t$为TD-error,$R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a995ec8a11d5abfe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def TDLambda(episodes, V, N, E, gamma, alpha=1.0, l=0.5):\n",
    "    \"\"\"\n",
    "    :param episodes: 采样好的episode\n",
    "    :param V: 状态的价值函数应该是一个一维的向量\n",
    "    :param N: 状态的计数器应该是一个矩阵\n",
    "    :param gamma: 衰减因子\n",
    "    :return: null\n",
    "    \"\"\"\n",
    "    for episode in episodes:\n",
    "        for i in range(len(episode)):  #一个序列从后往前计算\n",
    "            (s, a, r, s_next) = episode[i]\n",
    "            delta = r + gamma*V[s_next]-V[s]\n",
    "            for e in E:\n",
    "                E[e] = gamma*l*E[e]\n",
    "                if e == s:\n",
    "                    E[e] = E[e] + 1\n",
    "            V[s] = V[s] + alpha*delta*E[s]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:11:48.243406Z",
     "start_time": "2025-01-05T02:11:48.238854Z"
    }
   },
   "id": "c4e1fa84a27da4cd",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用TD(lambda)方法计算MDP的状态价值为\n",
      " {'s1': -0.7546977657290456, 's2': -1.57085726206068, 's3': 1.3744922429932185, 's4': 9.631480542689186, 's5': 0}\n"
     ]
    }
   ],
   "source": [
    "timestep_max = 20\n",
    "# 采样1000次,可以自行修改\n",
    "episodes = sample(MDP, Pi_1, timestep_max, 100)\n",
    "gamma = 0.5\n",
    "V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "E = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\n",
    "TDLambda(episodes, V, N, E, gamma, alpha=0.5, l=0.5)\n",
    "print(\"使用TD(lambda)方法计算MDP的状态价值为\\n\", V)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-05T02:11:48.749912Z",
     "start_time": "2025-01-05T02:11:48.739339Z"
    }
   },
   "id": "f2df20e51042a35a",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3184999e2e776c80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
